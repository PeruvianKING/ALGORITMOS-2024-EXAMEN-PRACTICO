                ALGORITMOS EXAMEN PRÁCTICO

Autor:                            Login: 
           

La práctica fue realizada con el siguiente dispositivo ->
Máquina: Intel i5-1021oU (8) 1.60 GHz (8gb ram)
OS: Kubuntu 23.04 64 bits
Kernel: 6.2.0-33 generic

Introducción:
El objetivo de la presente práctica es llevar a cabo la implementación del algoritmo de 
ordenación (se sospecha que es quicksort) y demostrar empíricamente su complejidad computacional. 
Para ello implementé la función de auxOrd (recursiva) junto a la función ordenar. 
El fin del algoritmo es la ordenación de forma ascendente de un vector cuyos elementos estarán 
ordenados de tres formas distintas: ascendente, aleatoria o descendente.

Para comprobar el funcionamiento del algoritmo utilizo 
el siguiente test:

*** Test ***
Inicializacion aleatoria
-10,   6,   0,  -7,  -3,  -8,   5,   7,   5,  -9
ordenado? 0
Ordenacion
-10,  -9,  -8,  -7,  -3,   0,   5,   5,   6,   7
ordenado? 1

Inicializacion descendente
  9,   8,   7,   6,   5,   4,   3,   2,   1,   0
ordenado? 0
Ordenacion
  0,   1,   2,   3,   4,   5,   6,   7,   8,   9
ordenado? 1

En el test inicializamos un vector cuyos elementos son números pseudoaleatorios y verificamos
que la ordenación con el algoritmo funciona correctamente, junto con la ordenación de un vector
inicializado descendientemente.


La siguiente parte de la práctica consiste en medir los tiempos de ejecución de la función del
algoritmo para distintos vectores de tamaño n, empezando en n=500 hasta n=32000, y para tres 
diferentes situaciones iniciales: 
vector ordenado crecientemente, decrecientemente e inicializado de forma pseudoaleatoria.

Dadas las siguientes tablas podemos determinar la complejidad computacional del algoritmo, así como la 
verificación empírica de su complejidad mediante el uso de una cota sobreestimada y otra subestimada.
 
* Descripción de la tabla:
    - El (*) situado a la izquierda de la tabla indica que la medición de tiempos fue realizada más de 
        una vez dentro de un bucle (en este caso 1000 veces) y se cogió el tiempo medio por iteración, 
        debido a que el tiempo obtenido fue menor a 500 microsegundos.

    -Todas las mediciones de tiempos están expresadas en microsegundos (μs).


Ordenación con inicialización ascendente
           n            t(n)         t(n)/n^1.70       t(n)/n^1.95       t(n)/n^2.30
          500       691.00000000      0.01783318        0.00377126        0.00042840
         1000      2592.00000000      0.02058898        0.00366130        0.00032631
         2000     10387.00000000      0.02539446        0.00379736        0.00026553
         4000     40910.00000000      0.03078416        0.00387090        0.00021237
         8000    154638.00000000      0.03581484        0.00378696        0.00016301
        16000    541586.00000000      0.03860675        0.00343268        0.00011593
        32000   2104701.00000000      0.04617801        0.00345261        0.00009148

Cota subestimada = n^1.7
Cota sobrestimada = n^1.95
Cota ajustada = n^2.3

* Observaciones:
  El cociente entre nuestros tiempos y la cota subestimada es creciente, mientras que en el caso de 
  la cota sobreestimada es al revés, es decir, tienen tendencia decreciente. En la cota ajustada, la 
  constante a la que tiende la sucesión es 0.003..*


Ordenación con inicialización descendente
           n            t(n)         t(n)/n^1.70       t(n)/n^2.00       t(n)/n^2.30
          500       594.00000000      0.01532983        0.00237600        0.00036826
         1000      2311.00000000      0.01835692        0.00231100        0.00029094
         2000      9612.00000000      0.02349972        0.00240300        0.00024572
         4000     47442.00000000      0.03569939        0.00296512        0.00024628
         8000    186413.00000000      0.04317407        0.00291270        0.00019650
        16000    640425.00000000      0.04565245        0.00250166        0.00013709
        32000   2523671.00000000      0.05537039        0.00246452        0.00010970

Cota subestimada = n^1.7
Cota sobrestimada = n^2
Cota ajustada = n^2.3

* Observaciones:
  El cociente entre nuestros tiempos y la cota subestimada es creciente, mientras que en el caso 
  de la cota sobreestimada es al revés, es decir, tienen tendencia decreciente. En la cota ajustada, 
  la constante a la que tiende la sucesión es 0.002..*

* Anomalías:
  En n = 4000 y n = 8000 el resultado es ligeramente superior a lo esperado en el cociente de la cota ajustada.  


Ordenación con inicialización aleatoria
           n            t(n)         t(n)/n^1.00       t(n)/n^1.13       t(n)/n^1.30
(*)       500       145.72400000      0.29144800        0.12992568        0.04517209
(*)      1000       259.42400000      0.25942400        0.10568422        0.03265956
(*)      2000       567.26000000      0.28363000        0.10558890        0.02900303
         4000      1357.00000000      0.33925000        0.11541229        0.02817747
         8000      2831.00000000      0.35387500        0.11001406        0.02387388
        16000      6197.00000000      0.38731250        0.11003376        0.02122393
        32000     13689.00000000      0.42778125        0.11105862        0.01904044

Cota subestimada = n^1
Cota sobrestimada = n^1.13
Cota ajustada = n^1.3

* Observaciones:
  El cociente entre nuestros tiempos y la cota subestimada es creciente, mientras que en el caso de 
  la cota sobreestimada es al revés, es decir, tienen tendencia decreciente. En la cota ajustada, la 
  constante a la que tiende la sucesión es 0.1..*

---Conclusiones--- 
En la presente práctica se han mostrado tres casos iniciales, los cuales difieren entre sí en tiempos de
ejecución. Como se puede observar en la tablas anteriormente incluídas, el algoritmo de ordenación
es más lento cuando el vector sobre el que se aplica ha sido inicializado ascendentemente, en este caso
la complejidad sería aproximadamente de O(n^1.95). En el caso de los vectores ordenados descendentemente 
podemos decir que su complejidad oscilaría alrededor de O(n^2), la cual se asemeja mucho a la obtenida
de vectores ascendentes, por lo que tiene sentido que sus tiempos se asemejen, aunque los tiempos medidos en
los ascendentes son mayores, por lo que probablemente la complejidad real en el caso de los vectores ascendentes
sea ligeramente mayor a la que obtuvimos. En el último caso, vectores inicializados con números pseudoaleatorios,
podemos observar que la complejidad es aproximadamente O(n^1.13).

En resumen, el algoritmo objeto de estudio, tiene una complejidad en el peor caso de O(n^2) aproximadamente,
estos son los casos en los que el vector ya está ordenado o está en orden descendente. Por otro lado, podemos
concluír que este algoritmo es tremendamente eficiente en aquellos casos cuyos vectores tienen elementos ordenados
de manera aleatoria, ya que la complejidad resultante en nuestras mediciones es de O(n^1.13) que podemos aproximar
a O(nlogn). Estas conclusiones pueden tener sentido, ya que si lo comparamos con nuestras sospechas iniciales
(algoritmo de ordenación quicksort), los resultados se asemejan mucho a este.
